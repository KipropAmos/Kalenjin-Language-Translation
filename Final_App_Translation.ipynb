{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5rGCRZdYvfe",
        "outputId": "ffb20007-12d2-4f47-d304-b7949d39f86f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text==2.8.*\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.46.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.25.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.2 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1hwDh9s1ZP"
      },
      "source": [
        "##  Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7PCjt709-91",
        "outputId": "326fff47-b36b-4c8e-b363-86dc7025abcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "print(tf.__version__)#to check the tensorflow version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY6rStF8huvw"
      },
      "source": [
        "##  Shapechecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTMhMqrChhVV"
      },
      "source": [
        "Function to prevent loading of data of wrong shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCF51Lophw-D"
      },
      "outputs": [],
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7oYNhBitA8V"
      },
      "source": [
        "##  Loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6rq-9h7-LVk"
      },
      "outputs": [],
      "source": [
        "# Loading the datasets\n",
        "english = pd.read_csv('/content/english.txt', sep='delimiter', engine = 'python', header=None)\n",
        "kiuk = pd.read_csv('/content/Kikuyu.txt', sep='delimiter', engine = 'python', header=None)\n",
        "kale = pd.read_csv('/content/kale.txt', sep='delimiter',  engine = 'python', header=None)\n",
        "luo = pd.read_csv('/content/luo.txt', sep='delimiter', engine = 'python', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXmwhiAZtO7C"
      },
      "source": [
        "##  Previewing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6LnmJjnt3PE",
        "outputId": "6374dc49-ffc6-43e8-923b-032e7226ef4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset has 176 rows and 1 columns\n",
            "The dataset has 176 rows and 1 columns\n",
            "The dataset has 176 rows and 1 columns\n",
            "The dataset has 176 rows and 1 columns\n"
          ]
        }
      ],
      "source": [
        "# print the shape of the various datasets\n",
        "files = [english, kiuk, luo, kale]\n",
        "dataset_names = ['English', 'Kikuyu', 'Luo', 'Kalenjin']\n",
        "for file in files:\n",
        "  #for index in range(len(dataset_names)):\n",
        "    rows, columns = file.shape\n",
        "    print(f'The dataset has {rows} rows and {columns} columns')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItSP4diOv_-v"
      },
      "source": [
        "##  Pre_processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23vUJSRhxhsn"
      },
      "outputs": [],
      "source": [
        "# (Unicode is the universal character encoding used to process, store and facilitate the interchange of text data in any language \n",
        "# while ASCII is used for the representation of text such as symbols, letters, digits, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKESEW4qxFGo"
      },
      "source": [
        "Preprocessing steps includes\n",
        "\n",
        "- Converting the unicode file to ascii\n",
        "- Creating a space between a word and the punctuation following it\n",
        "eg: “he is a boy.” => “he is a boy .” Reference\n",
        "- Replacing everything with space except (a-z, A-Z, “.”, “?”, “!”, “,”)\n",
        "- Adding a start and an end token to the sentence so that the model know when to start and stop predicting.\n",
        "- Removing the accents\n",
        "- Cleaning the sentences\n",
        "- Return word pairs in the format: [ENGLISH, LUO]\n",
        "- Creating a word -> index mapping (e.g,. 'Further' -> 5) and vice-versa. (e.g., 5 -> 'Further' ) for each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ns6OegFPV-j"
      },
      "outputs": [],
      "source": [
        "# Creating an index column for Kalenjin file\n",
        "kale['index_col'] = kale.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LYMqkFVw8Oe"
      },
      "outputs": [],
      "source": [
        "# Creating an index column for Kalenjin file\n",
        "english['index_col'] = kale.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvDFhp9CObTv"
      },
      "outputs": [],
      "source": [
        "# Joining the English and Kalenjin file with the Index column\n",
        "df_kale = pd.merge(english, kale, on = 'index_col')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnP8rk38PtsD"
      },
      "outputs": [],
      "source": [
        "# Renaming the Kalenjin Columns\n",
        "df_kale.head()\n",
        "df_kale.columns = ['feature', 'index', 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhVBZIzUQow5"
      },
      "outputs": [],
      "source": [
        "# Dropping the Index column in the Kalenjjin file\n",
        "df_kale.columns\n",
        "df_kale = df_kale.drop(columns = ['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CIEp-CbeQypJ",
        "outputId": "eeb0494c-2360-49c4-b055-85c49733a4b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5dd6e39c-805d-4240-b574-ffc6402c6810\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 Blessed are they that keep his testimonies, ...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 They also do no iniquity: they walk in his w...</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 Thou hast commanded us to keep thy precepts ...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5 O that my ways were directed to keep thy sta...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dd6e39c-805d-4240-b574-ffc6402c6810')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dd6e39c-805d-4240-b574-ffc6402c6810 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dd6e39c-805d-4240-b574-ffc6402c6810');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             feature  \\\n",
              "0  Blessed are the undefiled in the way, who walk...   \n",
              "1  2 Blessed are they that keep his testimonies, ...   \n",
              "2  3 They also do no iniquity: they walk in his w...   \n",
              "3  4 Thou hast commanded us to keep thy precepts ...   \n",
              "4  5 O that my ways were directed to keep thy sta...   \n",
              "\n",
              "                                              target  \n",
              "0  Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1  Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2  Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3  Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4  Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Displaying the first rows on the Kalenjin file\n",
        "df_kale.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "pi7fEmJ4RKrg",
        "outputId": "a49a2b9c-6932-4b09-fbb6-858b8c4386b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-761ed663-d19a-4c62-8b4e-2d0bfd6ee4d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blessed are they that keep his testimonies, a...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They also do no iniquity: they walk in his ways.</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thou hast commanded us to keep thy precepts d...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O that my ways were directed to keep thy stat...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-761ed663-d19a-4c62-8b4e-2d0bfd6ee4d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-761ed663-d19a-4c62-8b4e-2d0bfd6ee4d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-761ed663-d19a-4c62-8b4e-2d0bfd6ee4d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             feature  \\\n",
              "0  Blessed are the undefiled in the way, who walk...   \n",
              "1   Blessed are they that keep his testimonies, a...   \n",
              "2   They also do no iniquity: they walk in his ways.   \n",
              "3   Thou hast commanded us to keep thy precepts d...   \n",
              "4   O that my ways were directed to keep thy stat...   \n",
              "\n",
              "                                              target  \n",
              "0  Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1  Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2  Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3  Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4  Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing the numbers at the beginning of the feature column\n",
        "df_kale['feature'] = df_kale['feature'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "JxzDPMwJc3yR",
        "outputId": "e402e29b-3d96-456d-ca0c-0d6013f9d5b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7a905857-0190-4143-b997-d49ad6e3a03e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blessed are they that keep his testimonies, a...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They also do no iniquity: they walk in his ways.</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thou hast commanded us to keep thy precepts d...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O that my ways were directed to keep thy stat...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>My tongue shall speak of thy word: for all th...</td>\n",
              "      <td>Ingotien ng’elyeptanyu agobo ng’olyondeng’ung’...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>Let thine hand help me; for I have chosen thy...</td>\n",
              "      <td>Ingochobok eung’ung’ kotoreta; Amu kialewen ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>I have longed for thy salvation, O Lord; and ...</td>\n",
              "      <td>Kigoama emosto agobo yetuneng’ung’, ee Jehovah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>Let my soul live, and it shall praise thee; a...</td>\n",
              "      <td>Ingosob sobondanyu, si kolosun; Ak ingotoreta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>I have gone astray like a lost sheep; seek th...</td>\n",
              "      <td>Kiabetote ko u kechiriet ne betot; cheng’ kibo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>176 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a905857-0190-4143-b997-d49ad6e3a03e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a905857-0190-4143-b997-d49ad6e3a03e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a905857-0190-4143-b997-d49ad6e3a03e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               feature  \\\n",
              "0    Blessed are the undefiled in the way, who walk...   \n",
              "1     Blessed are they that keep his testimonies, a...   \n",
              "2     They also do no iniquity: they walk in his ways.   \n",
              "3     Thou hast commanded us to keep thy precepts d...   \n",
              "4     O that my ways were directed to keep thy stat...   \n",
              "..                                                 ...   \n",
              "171   My tongue shall speak of thy word: for all th...   \n",
              "172   Let thine hand help me; for I have chosen thy...   \n",
              "173   I have longed for thy salvation, O Lord; and ...   \n",
              "174   Let my soul live, and it shall praise thee; a...   \n",
              "175   I have gone astray like a lost sheep; seek th...   \n",
              "\n",
              "                                                target  \n",
              "0    Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1    Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2    Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3    Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4    Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  \n",
              "..                                                 ...  \n",
              "171  Ingotien ng’elyeptanyu agobo ng’olyondeng’ung’...  \n",
              "172  Ingochobok eung’ung’ kotoreta; Amu kialewen ko...  \n",
              "173  Kigoama emosto agobo yetuneng’ung’, ee Jehovah...  \n",
              "174  Ingosob sobondanyu, si kolosun; Ak ingotoreta ...  \n",
              "175  Kiabetote ko u kechiriet ne betot; cheng’ kibo...  \n",
              "\n",
              "[176 rows x 2 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_kale['feature'] = df_kale['feature'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale['target'] = df_kale['target'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bv0qJn7aYot"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"df_kale.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npB5UQG4epUy"
      },
      "outputs": [],
      "source": [
        "inp = df_kale['target'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oa6ivQBcE4w"
      },
      "outputs": [],
      "source": [
        "targ = df_kale['feature'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPz-5B-ve5OG"
      },
      "source": [
        "##  Creating tf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skpuhugPjU6G"
      },
      "source": [
        "Creating a tf.data.Dataset of strings that shuffles and batches them efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn3o0fzUez0p",
        "outputId": "5704c9eb-b01c-468e-9ccf-047ff8dfbc37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tells TensorFlow to create a buffer of at most buffer_size elements, and a background thread to fill that buffer in the background\n",
        "BUFFER_SIZE = len(inp)\n",
        "\n",
        "# Number of samples to be feed into the neural network\n",
        "BATCH_SIZE = 3\n",
        "\n",
        "# Creating the dataset and shuffling it \n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX98Wa1bHlsE",
        "outputId": "ae6e88ad-7984-4819-80d0-354d0000a9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'Iweech konyekyuk si mageer che ma nyolu. Ak isooba eng\\xe2\\x80\\x99 ortinweguk.'\n",
            " b'Ikweng\\xe2\\x80\\x99e kiboitiondeng\\xe2\\x80\\x99ung\\xe2\\x80\\x99 togeng\\xe2\\x80\\x99ung\\xe2\\x80\\x99; Ak ineta ng\\xe2\\x80\\x99atutiguk.'\n",
            " b'Kaimenitu konyekyuk agobo ng\\xe2\\x80\\x99olyondeng\\xe2\\x80\\x99ung\\xe2\\x80\\x99, Ye amwae ale, Igaigaa au?'], shape=(3,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b' Turn away mine eyes from beholding vanity; and quicken thou me in thy way.'\n",
            " b' Make thy face to shine upon thy servant; and teach me thy statutes.'\n",
            " b' Mine eyes fail for thy word, saying, When wilt thou comfort me?'], shape=(3,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kCgCCkofZql"
      },
      "source": [
        "##  Text processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxJjvOzYfeSA"
      },
      "source": [
        "### i) Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbsccb0GlpPP"
      },
      "source": [
        "Since the model is dealing with multilingual text with a limited vocabulary standardization of the text is crucial. Steps;\n",
        "1.  Unicode normalization to split accented characters\n",
        "2.  replace compatibility characters with their ASCII equivalents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncfk-gznZ78G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qimZmadJge4G"
      },
      "outputs": [],
      "source": [
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaXEDlsSfcDu",
        "outputId": "3697e553-7d85-4d1f-ba63-5ccbb0f5d9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'Kiacheng\\xe2\\x80\\x99in eng\\xe2\\x80\\x99 muguleldanyu tugul'\n",
            "b'Kiacheng\\xe2\\x80\\x99in eng\\xe2\\x80\\x99 muguleldanyu tugul'\n"
          ]
        }
      ],
      "source": [
        "# example of a text normalized and uni encoded\n",
        "sample_text = tf.constant('Kiacheng’in eng’ muguleldanyu tugul')\n",
        "\n",
        "print(sample_text.numpy())\n",
        "print(tf_text.normalize_utf8(sample_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvbI7A7hgaBj"
      },
      "outputs": [],
      "source": [
        "# Unicode normalization \n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTC9ec0vg2w9",
        "outputId": "067b9dd5-4b38-418a-80af-c41644cd2613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kiacheng’in eng’ muguleldanyu tugul\n",
            "[START] kiachengin eng muguleldanyu tugul [END]\n"
          ]
        }
      ],
      "source": [
        "# Priniting an example of the original text\n",
        "print(sample_text.numpy().decode())\n",
        "\n",
        "# printing the text afterunicode normalization\n",
        "print(tf_lower_and_split_punct(sample_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcG-ObPshAmb"
      },
      "outputs": [],
      "source": [
        "# Extracting and coverting input text to sequences of tokens\n",
        "# max_vocab_size limit RAM usage during the initial scan of the training corpus to discover the vocabulary.\n",
        "max_vocab_size = 25000 \n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euDjRG9ZhEPH",
        "outputId": "5f3fb5c9-d0d3-4c05-95a5-46d952ef3564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', ',', 'ak', 'eng', 'amu', 'ngatutiguk']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reading one epoch of the training data with the adapt method \n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdrMYNIzhKO1",
        "outputId": "92cd3c3a-7da9-4a99-d599-3d4537d3b10c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'thy', '[START]', '[END]', '.', 'i', ',', 'me', 'and']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using the Kalenjin TextVectorization layer to build the English layer with .adapt() method\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGz0sQG2hRI9",
        "outputId": "5085a635-37df-49ac-f330-70ff38c4694d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[  2, 394,  59,  14, 254,  11, 133, 226,   4,   6],\n",
              "       [  2, 414,  27, 197,   6,  30,   9,   4,   3,   0],\n",
              "       [  2, 157,  59,  33,  12,   5,  37, 481,  36,   5]])>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using the layers created to convert a batch of strings into a batch of token IDs\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "AMFb-oOlhSBZ",
        "outputId": "fed79aa1-d71f-4198-eb7a-22eb3f253780"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Applying the token IDs that are zero-padded that can be turned into a mask\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W34FhUHQhXYB"
      },
      "outputs": [],
      "source": [
        "# Defining constants for the model\n",
        "# Embedding layer enables us to convert each word into a fixed length vector of defined size\n",
        "embedding_dim = 512\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfs8q1paheh8"
      },
      "source": [
        "##  The encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfGvULMIxdyM"
      },
      "source": [
        "The first thing to do is build the encoder. The process is as follows:\n",
        "\n",
        "1. Taking a list of token IDs. \n",
        "\n",
        "2. Using the embedding vector for each token.\n",
        "\n",
        "3. Processessing the embeddings into a new sequence "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF5tj3-LhaoV"
      },
      "outputs": [],
      "source": [
        "# Applying the  list of token IDs\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCGAOYOKhhyU",
        "outputId": "b9b12e15-17e4-4886-a8cc-5965a558d16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch, shape (batch): (3,)\n",
            "Input batch tokens, shape (batch, s): (3, 15)\n",
            "Encoder output, shape (batch, s, units): (3, 15, 1024)\n",
            "Encoder state, shape (batch, units): (3, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86t5JagIh0jA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS6yGwmvh7wE"
      },
      "source": [
        "##  The attention head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyENzTlKzQJL"
      },
      "source": [
        "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNl8GtARh-_j"
      },
      "outputs": [],
      "source": [
        "# The BahdanauAttention class handles the weight matrices in a pair of dense layers and calls the builtin implementation\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8lEay96iFaK"
      },
      "source": [
        "### i) Attention head layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3D4q8LqiDDG"
      },
      "outputs": [],
      "source": [
        "# Creating a BahdanauAttention layer\n",
        "attention_layer = BahdanauAttention(units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXZOMPBeiLJT",
        "outputId": "746801c7-c1da-4ce9-c6cf-4b695d8b87e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([3, 15])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Excluding the padding\n",
        "(example_tokens != 0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAIY77zQiOLi",
        "outputId": "9249cd37-8ea5-4665-8ebe-39349d30fa29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (3, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (3, 2, 15)\n"
          ]
        }
      ],
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "S3sxTlpbiSbN",
        "outputId": "e2773ea2-fd27-43f4-d576-119e56606bfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# attention weights across the sequences at t=0\n",
        "# t is used for slicing, for selecting different parts of the data.\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtKEg3H-iWHO",
        "outputId": "61229663-1e1b-4d14-8b56-ad88231d1acd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([3, 2, 15])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Displaying the shape of the attention weights\n",
        "attention_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLIfNuypiZs1"
      },
      "outputs": [],
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-EMQsamifCP"
      },
      "source": [
        "### ii) Toogle code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kC-M1cWiihZl",
        "outputId": "13b9f1ea-97e2-4bc7-d53d-3f48add3edfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9b136f6290>]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting attention weights\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aJ1-TaViluf"
      },
      "source": [
        "### iii) The decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRld9r7zaAao"
      },
      "source": [
        "The decoder generates predictions for the next output token.\n",
        "1. The decoder receives the complete encoder output.\n",
        "\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "\n",
        "4. It combines the RNN output and the context vector to generate the attention vector.\n",
        "\n",
        "5. It generates logit predictions for the next token based on the attention vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2w6cOE_ijbo"
      },
      "outputs": [],
      "source": [
        "# Decoder class and its initializer creates all the necessary layers.\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    #  Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BJX5xOzi2Ir"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import typing\n",
        "from typing import Any, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7aMUkaXisc8"
      },
      "outputs": [],
      "source": [
        "# Applying the call method for this layer which  takes and returns multiple tensors.\n",
        "# Organizing those into simple container classes.\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb46LHidjFpW"
      },
      "outputs": [],
      "source": [
        "# Implementing the call method\n",
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  if state is not None:\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyzKL0jmjnK3"
      },
      "outputs": [],
      "source": [
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAUFYYjFjJoL"
      },
      "outputs": [],
      "source": [
        "# Implementing  of the decoder \n",
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOCS-aMZjMex"
      },
      "outputs": [],
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8tAW2qgjPX-",
        "outputId": "88dfb37b-880d-4959-e41b-eca53cc209e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits shape: (batch_size, t, output_vocab_size) (3, 1, 486)\n",
            "state shape: (batch_size, dec_units) (3, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0Sr9yCejyGR"
      },
      "outputs": [],
      "source": [
        "# Sampling a token with the logits\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_36L8TSj3RB",
        "outputId": "9a7c6d12-06ab-463e-925f-b8b1783fbe89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['speak'],\n",
              "       ['continually'],\n",
              "       ['voice']], dtype='<U14')"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Decoding the token as the first word of the output\n",
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-K4tG9kkEDX"
      },
      "outputs": [],
      "source": [
        "# Applying the same enc_output, mask and sampled tokens as new tokens.\n",
        "\n",
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OSaBZEQkLex",
        "outputId": "834eeab9-85d8-483a-ff21-a9be08405740"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['man'],\n",
              "       ['rivers'],\n",
              "       ['hath']], dtype='<U14')"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generating a second set of logits using the decoder\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ng_jdDekShC"
      },
      "source": [
        "##  Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_FDJCl_gkoW"
      },
      "source": [
        "To train the model we'll follow the following steps:\n",
        "\n",
        "1. A loss function and optimizer to perform the optimization.\n",
        "\n",
        "2. A training step function defining how to update the model for each input/target batch.\n",
        "\n",
        "3. A training loop to drive the training and save checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjXzI_1k8Du"
      },
      "source": [
        "### i) Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2IcOwyKj8At"
      },
      "outputs": [],
      "source": [
        "# Implementing the loss function and optimizer to perform the optimization.\n",
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaII-2g3lLmO"
      },
      "source": [
        "### ii) Implementing the training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTsZs60qkcOQ"
      },
      "outputs": [],
      "source": [
        "# Implementing a model class, the training process will be implemented as the train_step method \n",
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR_m1uF4kfzg"
      },
      "outputs": [],
      "source": [
        "# Getting a batch of input_text, target_text from the tf.data.Dataset.\n",
        "def _preprocess(self, input_text, target_text):\n",
        "  self.shape_checker(input_text, ('batch',))\n",
        "  self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-oqNm4zlZZg"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyD9YlGlkhiY"
      },
      "outputs": [],
      "source": [
        "# Applying the _train_step method\n",
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3mVALlPmbLQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._train_step = _train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOehrED0md0-"
      },
      "outputs": [],
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNBAPnZwmfFf"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__19iWQPmiwa"
      },
      "source": [
        "### iii) Test the training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf7vv-CqmnDy"
      },
      "outputs": [],
      "source": [
        "# Building a TrainTranslator and configuring it for training using the Model.compile method\n",
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7YpBQGgmpeq",
        "outputId": "522eda98-b206-47ba-c962-b33b50da3e0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.186208623900494"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing the train_step model\n",
        "np.log(output_text_processor.vocabulary_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65UeaPVZmsKv"
      },
      "outputs": [],
      "source": [
        "# Applying the tf.function-wrapped _tf_train_step, to maximize performance while training\n",
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pHTE-FLmw4s"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrDSsPrsmzC5"
      },
      "outputs": [],
      "source": [
        "translator.use_tf_function = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_uZLIwbnChM",
        "outputId": "5cf6e6a8-8703-420d-be61-9e522a79959a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.8170986>}"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tracing the function\n",
        "translator.train_step([example_input_batch, example_target_batch])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNHqOaK0nDRa",
        "outputId": "feff1c38-33ab-4832-da09-b978cf2f029d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.675062>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.4519854>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.9306383>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.5844378>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.8389463>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.099932>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.2971683>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.0378928>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.9476435>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.7302275>}\n",
            "\n",
            "CPU times: user 32.7 s, sys: 846 ms, total: 33.6 s\n",
            "Wall time: 38.3 s\n"
          ]
        }
      ],
      "source": [
        "# Printing out the Batch loss of our model\n",
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOl01ZMFnOsT",
        "outputId": "160e77fb-5928-4033-9823-d56e5baedd12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "................"
          ]
        }
      ],
      "source": [
        "# Plotting our batch losses\n",
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPHSFGQwnSCL"
      },
      "outputs": [],
      "source": [
        "# Building another model to train\n",
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY3YQsfan-O_"
      },
      "source": [
        "### iv) Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNhN4bJHoBSi"
      },
      "outputs": [],
      "source": [
        "# Training a couple of epochs by applying the callbacks.Callback method\n",
        "# to collect the history of batch losses\n",
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgA08TIfoEvf"
      },
      "outputs": [],
      "source": [
        "# Displaying the batch loss using 15 epochs \n",
        "train_translator.fit(dataset, epochs=15,\n",
        "                     callbacks=[batch_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPsGUSJkqWi6"
      },
      "outputs": [],
      "source": [
        "# Plotting the epochs\n",
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "079uq-8ZqcCF"
      },
      "source": [
        "##  Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pee0S4sxqbZL"
      },
      "outputs": [],
      "source": [
        "# Executing the full text => texttranslation\n",
        "# This is by inverting the text => token IDsmapping provided by the output_text_processor\n",
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1zOGt-eqjgk"
      },
      "outputs": [],
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDZbxQB3qp3H"
      },
      "source": [
        "### i) Convert IDs to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KphbGFO7q9Jc"
      },
      "outputs": [],
      "source": [
        "# Implementing the tokens_to_text which converts from token IDs to human readable text.\n",
        "def tokens_to_text(self, result_tokens):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCmFMywfqsO7"
      },
      "outputs": [],
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po_ThN89rEWF"
      },
      "outputs": [],
      "source": [
        "# Inputting some random token IDs and see what it generates (example)\n",
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4muFfacgrL9E"
      },
      "source": [
        "### ii) Sample from the decoder's predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYIHYKfJrKOz"
      },
      "outputs": [],
      "source": [
        "# Taking the decoder's logit outputs and samples token IDs from the distribution\n",
        "def sample(self, logits, temperature):\n",
        "  shape_checker = ShapeChecker()\n",
        "  # 't' is usually 1 here.\n",
        "  shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "\n",
        "  shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VteBiIP0rS8S"
      },
      "outputs": [],
      "source": [
        "Translator.sample = sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNDKC8OyrZ2S"
      },
      "outputs": [],
      "source": [
        "# Random inputs (example)\n",
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Nr3PkorXYF"
      },
      "source": [
        "### iii) Implement translation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spIWGzjNrVsC"
      },
      "outputs": [],
      "source": [
        "# Taking the results into python lists before joining them  using tf.concat into tensors.\n",
        "# This unfolds the graph out to max_length iterations.\n",
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "\n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSzYfqdFripi"
      },
      "outputs": [],
      "source": [
        "Translator.translate = translate_unrolled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gncwm1c6rlmS"
      },
      "outputs": [],
      "source": [
        "# Running a simple input to view the translation\n",
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'Boiboen che igesunotgei eng’ oret.', # \"Blessed are the undefiled in the way.\"\n",
        "    'Kilosu Jehovah', # \"I have gone astray like a lost sheep\"\n",
        "])\n",
        "\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uezLA_MYuRWg"
      },
      "source": [
        "##  Visualize the process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wP-I_t4uQQJ"
      },
      "outputs": [],
      "source": [
        "# Calculating the sum of the attention over the input which should return all ones.\n",
        "a = result['attention'][0]\n",
        "\n",
        "print(np.sum(a, axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bakU42xPuXL1"
      },
      "outputs": [],
      "source": [
        "# The attention distribution for the first output step of the first example\n",
        "# It is focused than it was in the untrained model\n",
        "_ = plt.bar(range(len(a[0, :])), a[0, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQVQ02YQubSE"
      },
      "outputs": [],
      "source": [
        "# There is some rough alignment between the input and output words\n",
        "plt.imshow(np.array(a), vmin=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Ff_LgNui5O"
      },
      "source": [
        "### i) Labelled attention plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLFvBR__u5hv"
      },
      "outputs": [],
      "source": [
        "# Visualizing the attention plots.\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtk8wPMduev-"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "plot_attention(result['attention'][i], input_text[i], result['text'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlq2VEE3p-fi"
      },
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2cHjk7tp9gT"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_translate(self, input_text):\n",
        "  return self.translate(input_text)\n",
        "\n",
        "Translator.tf_translate = tf_translate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tcB73funwpf"
      },
      "source": [
        "##  Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj2G7roFn1hN"
      },
      "source": [
        "a). Did we have the right data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjuvLmo6bF01"
      },
      "source": [
        "b). Do we need other data to answer our question?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS0AjWVvbaPM"
      },
      "source": [
        "c) Did we have the right question?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL_GNDpjYTQu"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5VMXz4ltYRWv",
        "outputId": "fba00b9d-b416-4caf-856c-d0280ee04ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydeck\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck) (7.7.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pydeck) (1.21.6)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck) (2.11.3)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.15.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck) (5.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck) (21.3)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck) (23.1.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck) (1.0.0)\n",
            "Collecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck) (0.1.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck) (5.4.8)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck) (1.5.5)\n",
            "Collecting tornado>=6.1\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (0.18.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck) (1.1.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck) (4.10.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck) (2.8.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (2.15.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (4.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (4.11.4)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck) (1.15.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (5.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipykernel>=5.1.2->pydeck) (3.0.9)\n",
            "Installing collected packages: tornado, prompt-toolkit, jupyter-client, ipython, ipykernel, pydeck\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.6.4 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.15.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ipykernel-6.15.0 ipython-7.34.0 jupyter-client-7.3.4 prompt-toolkit-3.0.29 pydeck-0.7.1 tornado-6.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit==0.75.0\n",
            "  Downloading streamlit-0.75.0-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (21.3)\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (6.1)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (2.23.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (4.2.4)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (1.3.5)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (0.8.1)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (4.2.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (1.5.1)\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (7.1.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (6.0.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (3.17.3)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (1.21.6)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.75.0) (0.7.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.75.0) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.75.0) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.75.0) (0.11.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.75.0) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.75.0) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.75.0) (4.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.75.0) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.75.0) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.75.0) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==0.75.0) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit==0.75.0) (2022.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==0.75.0) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.75.0) (7.7.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.75.0) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.75.0) (6.15.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (1.5.5)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (7.34.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (1.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (5.4.8)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (23.1.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.1.3)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (7.3.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (3.0.29)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.18.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (2.6.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (1.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (5.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==0.75.0) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (2.15.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.75.0) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (1.8.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (5.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.75.0) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit==0.75.0) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.75.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.75.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.75.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.75.0) (2.10)\n",
            "Building wheels for collected packages: blinker, validators\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=365cfa04d3e93ccbff470ddb560c5d689783342517b8f72797c149e2d5682291\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=3032f8ad88f84544b75a0c42f4a23b7266b7e2028a495551a629ad8eaff61109\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built blinker validators\n",
            "Installing collected packages: smmap, gitdb, watchdog, validators, toml, gitpython, blinker, base58, streamlit\n",
            "Successfully installed base58-2.1.1 blinker-1.4 gitdb-4.0.9 gitpython-3.1.27 smmap-5.0.0 streamlit-0.75.0 toml-0.10.2 validators-0.20.0 watchdog-2.1.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=d10296d2bd3b8915fa82141afe928aa4ccc3f4517fc664ea60de6adfeb6fee04\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.75.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.10.0-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.7.1)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.27)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Collecting rich\n",
            "  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.6)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.20.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.9)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.8.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2022.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (6.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.7.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.34.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.4.8)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.3.4)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.0.29)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.6.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.15.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: commonmark, rich, pympler, streamlit\n",
            "  Attempting uninstall: streamlit\n",
            "    Found existing installation: streamlit 0.75.0\n",
            "    Uninstalling streamlit-0.75.0:\n",
            "      Successfully uninstalled streamlit-0.75.0\n",
            "Successfully installed commonmark-0.9.1 pympler-1.0.1 rich-12.4.4 streamlit-1.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit-option-menu\n",
            "  Downloading streamlit_option_menu-0.3.2-py3-none-any.whl (712 kB)\n",
            "\u001b[K     |████████████████████████████████| 712 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.7/dist-packages (from streamlit-option-menu) (1.10.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (6.0.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.11.4)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.7.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (12.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (21.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (7.1.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.1.27)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.4)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.1.9)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.5.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.17.3)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.3.5)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.20.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.4)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (6.1)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.8.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.13.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (0.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (2.11.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit>=0.63->streamlit-option-menu) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit>=0.63->streamlit-option-menu) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit>=0.63->streamlit-option-menu) (3.8.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (0.18.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit>=0.63->streamlit-option-menu) (2022.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit>=0.63->streamlit-option-menu) (1.15.0)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (6.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (5.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (7.7.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.1.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (7.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (1.5.5)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (7.34.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (23.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (5.4.8)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (3.0.29)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (4.4.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (3.6.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (2.15.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (5.6.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (5.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit>=0.63->streamlit-option-menu) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit>=0.63->streamlit-option-menu) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit>=0.63->streamlit-option-menu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit>=0.63->streamlit-option-menu) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit>=0.63->streamlit-option-menu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit>=0.63->streamlit-option-menu) (3.0.4)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich->streamlit>=0.63->streamlit-option-menu) (0.9.1)\n",
            "Installing collected packages: streamlit-option-menu\n",
            "Successfully installed streamlit-option-menu-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ipykernel>=5.1.2\n",
        "!pip install pydeck\n",
        "!pip install streamlit==0.75.0\n",
        "!pip install pyngrok\n",
        "!pip install streamlit -q\n",
        "!pip install streamlit --upgrade\n",
        "!pip install streamlit-option-menu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k7ASCARZHGj"
      },
      "source": [
        "## App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9hEQcRVYPIu",
        "outputId": "45b6684a-c3e4-4d1f-cc1b-0b0b7dc03929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing finalLHTranslation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile finalLHTranslation.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.figure_factory as ff\n",
        "from streamlit_option_menu import option_menu\n",
        "from textblob import TextBlob \n",
        "import tensorflow as tf\n",
        " \n",
        " \n",
        "\n",
        "# let's do the navigation bar first\n",
        "\n",
        "selected = option_menu(\n",
        "      menu_title= None, options=['Home','Features', 'About'], icons =['house','book','boxes'],menu_icon='cast', default_index=0, orientation = 'horizontal'\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "# setting containers\n",
        "header = st.container()\n",
        "translation = st.container()\n",
        "dataset = st.container()\n",
        "features = st.container()\n",
        "modelTraining = st.container()\n",
        "\n",
        "# setting columns within the header container\n",
        "with header:\n",
        "  col1, col2 = st.columns([1,6])\n",
        "\n",
        "  with col1:\n",
        "    st.image(\n",
        "    \"https://cdn0.iconfinder.com/data/icons/joker-circus-by-joker2011-d3g8h6s/256/lion.png\", width=100,)\n",
        "\n",
        "  with col2:\n",
        "    st.markdown(\"<h1 style='text-align: left; color: Orange;'> Lion Heart Translation</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "  # st.image(\n",
        "  #   \"https://cdn0.iconfinder.com/data/icons/joker-circus-by-joker2011-d3g8h6s/256/lion.png\", width=100,)\n",
        "  # st.markdown(\"<h1 style='text-align: center; color: Purple;'> Lion Heart Translation</h1>\", unsafe_allow_html=True)\n",
        "  # st.text(\"Lion Heart Translation is an App who's main porpose is to translate Kenyan local languages, that is Kalenjin, Luo and Kikuyu to English and vise versa. It's Using a tensorflow neural network model in order to do this.\")\n",
        "with st.expander(\"Pick out a theme of your liking?\"):\n",
        "  THEMES = [\n",
        "    \"light\",\n",
        "    \"dark\",\n",
        "    \"green\",\n",
        "    \"blue\",]\n",
        "  GITHUB_OWNER = \"streamlit\"\n",
        "\n",
        "\n",
        "\n",
        "# modeling ie translation code\n",
        "if selected == 'Home':\n",
        "  with translation:\n",
        "    from textblob import TextBlob \n",
        "    import spacy\n",
        "    from gensim.summarization import summarize\n",
        "    sp = spacy.load('en_core_web_sm')\n",
        "    from spacy import displacy\n",
        " \n",
        "    # Add selectbox in streamlit\n",
        "    st.markdown(\"\"\"<span style=\"word-wrap:break-word;\">Lion Heart Translation is an App who's main porpose is to translate Kenyan local languages, that is Kalenjin, Luo and Kikuyu to English and vise versa. It's Using a tensorflow neural network model in order to do this.</span>\"\"\", unsafe_allow_html=True)\n",
        "    option = st.selectbox(\n",
        "     'Which Local language would you like to translate to?',\n",
        "        ('none', 'Kikuyu', 'Kalenjin', 'Luo'))\n",
        "    st.write('You selected:', option)\n",
        "\n",
        "    def main():\n",
        "      text = st.text_area(\"Enter Text to translate here: \",\"lorem ipsum...\",key = \"<255>\")\n",
        "      if st.button(\"Translate\"):\n",
        "        input_text = tf.constant(text)\n",
        "        result = translator.translate(input_text = input_text)\n",
        "        show = result['text'][0].numpy().decode()\n",
        "        st.success(show)\n",
        "\n",
        "if __name__=='__main__':\n",
        "  main()\n",
        "\n",
        "\n",
        "if selected == 'About':\n",
        "  with dataset:\n",
        "    st.header('About')\n",
        "    st.markdown(\"\"\"<span style=\"word-wrap:break-word;\">The data used to build this model was obtained from a chapter in the Bible in each of the four languages.</span>\"\"\", unsafe_allow_html=True)\n",
        "    \n",
        "    # st.text('The data used to build this model was obtained from a chapter in the Bible in each of the four languages.')\n",
        "    st.text(\"here's what the Kalenjin looks side by side with it's English translation\")\n",
        "    kaleme = pd.read_csv('/content/kaleme.csv', sep='delimiter', engine = 'python', header=None)\n",
        "    st.write(kaleme.head(5))\n",
        "\n",
        "if selected == 'Features':\n",
        "  with features:\n",
        "    st.header('Features')\n",
        "    st.markdown('* **Hyperparameter tuning:** Here the user can tweak the model settings in pursuit of higher accuracy')\n",
        "    st.markdown('* **Language Dropdown:** Here the user can tweak the model settings in pursuit of higher accuracy')\n",
        "    st.markdown('* **Translation textbox:** Here the user can tweak the model settings in pursuit of higher accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNJf5P3BZDYd",
        "outputId": "274465d3-412b-4392-da08-8efbdd693438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-06-15 18:57:50.949 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.108.25:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 6.364s\n",
            "your url is: https://ninety-regions-relate-34-125-108-25.loca.lt\n",
            "2022-06-15 18:59:24.277 'pattern' package not found; tag filters are not available for English\n"
          ]
        }
      ],
      "source": [
        "! streamlit run finalLHTranslation.py & npx localtunnel --port 8501\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zlq2VEE3p-fi",
        "0tcB73funwpf"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}